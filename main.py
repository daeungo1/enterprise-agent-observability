"""FastAPI Chat Agent with LangGraph - Teacher-Student Quiz System with OpenTelemetry"""
import asyncio
import json
from contextlib import asynccontextmanager
from pathlib import Path
from typing import Optional, AsyncGenerator
from uuid import uuid4

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

# OpenTelemetry + Traceloop for LLM tracing
from traceloop.sdk import Traceloop
from opentelemetry import trace

from langchain_core.messages import HumanMessage

from config import AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT_NAME, OTEL_EXPORTER_OTLP_ENDPOINT
from graph import (
    create_graph, 
    QuizPhase,
)
from eval_background import evaluate_single

# Global
graph = None
# Session state storage per session (phase, difficulty, subject, etc.) / ì„¸ì…˜ë³„ ìƒíƒœ ì €ì¥
session_states = {}


# OpenTelemetry tracer
tracer = None

def setup_opentelemetry():
    """Initialize OpenTelemetry + Traceloop (LLM input/output capture) / OpenTelemetry + Traceloop ì´ˆê¸°í™”"""
    global tracer
    
    import os
    # Increase attribute length limit (default is too small for LLM messages) / Attribute ê¸¸ì´ ì œí•œ ëŠ˜ë¦¬ê¸°
    os.environ.setdefault("OTEL_ATTRIBUTE_VALUE_LENGTH_LIMIT", "65535")
    # Enable Traceloop content capture / Traceloop content ìº¡ì²˜ í™œì„±í™”
    os.environ.setdefault("TRACELOOP_TRACE_CONTENT", "true")
    
    # Initialize Traceloop - auto-instrument LangChain, OpenAI, etc. / Traceloop ì´ˆê¸°í™”
    # Create exporter to send to OTel Collector / exporterë¥¼ ì§ì ‘ ìƒì„±í•˜ì—¬ OTel Collectorë¡œ ì „ì†¡
    from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
    
    otlp_exporter = OTLPSpanExporter(
        endpoint=OTEL_EXPORTER_OTLP_ENDPOINT,
        insecure=True,
    )
    
    Traceloop.init(
        app_name="teacher-student-quiz",
        disable_batch=False,
        exporter=otlp_exporter,
    )
    
    tracer = trace.get_tracer(__name__)
    
    print(f"âœ… OpenTelemetry + Traceloop initialized!")
    print(f"   OTLP Endpoint: {OTEL_EXPORTER_OTLP_ENDPOINT}")
    
    return tracer


@asynccontextmanager
async def lifespan(app: FastAPI):
    global graph, tracer
    try:
        # Initialize OpenTelemetry / OpenTelemetry ì´ˆê¸°í™”
        tracer = setup_opentelemetry()
        
        graph = create_graph()
        print("âœ… LangGraph Teacher-Student Quiz Agent initialized!")
        print(f"   Endpoint: {AZURE_OPENAI_ENDPOINT}")
        print(f"   Deployment: {AZURE_OPENAI_DEPLOYMENT_NAME}")
    except Exception as e:
        print(f"âŒ Failed to initialize: {e}")
        raise e
    yield
    print("Shutting down...")


app = FastAPI(
    title="LangGraph Chat Agent",
    description="Chat agent powered by LangGraph and Azure OpenAI",
    version="1.0.0",
    lifespan=lifespan
)

# Static file serving / Static íŒŒì¼ ì„œë¹™
app.mount("/static", StaticFiles(directory=Path(__file__).parent / "static"), name="static")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None  # Session ID (creates new if not provided) / ì„¸ì…˜ ID


class ChatResponse(BaseModel):
    response: str
    session_id: str  # Session ID for client's next request / í´ë¼ì´ì–¸íŠ¸ê°€ ë‹¤ìŒ ìš”ì²­ì— ì‚¬ìš©í•  ì„¸ì…˜ ID


@app.get("/", response_class=HTMLResponse)
async def root():
    template_path = Path(__file__).parent / "templates" / "index.html"
    return template_path.read_text(encoding="utf-8")


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    if not graph:
        raise HTTPException(status_code=503, detail="Agent not initialized")
    
    try:
        # Handle session ID (create new if not provided) / ì„¸ì…˜ ID ì²˜ë¦¬
        session_id = request.session_id or str(uuid4())
        
        # Get or initialize session state / ì„¸ì…˜ ìƒíƒœ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ì´ˆê¸°í™”
        if session_id not in session_states:
            session_states[session_id] = {
                "phase": QuizPhase.SETUP,
                "difficulty": None,
                "subject": None,
                "round_count": 0,
            }
        
        current_state = session_states[session_id]
        user_input = request.message.strip()
        
        # Use LangGraph built-in checkpointer / LangGraph ë‚´ì¥ checkpointer ì‚¬ìš©
        config = {"configurable": {"thread_id": session_id}}
        
        # Process based on current phase / í˜„ì¬ phaseì— ë”°ë¥¸ ì²˜ë¦¬
        phase = current_state.get("phase", QuizPhase.SETUP)
        
        # Handle reset commands / ë¦¬ì…‹ ëª…ë ¹ ì²˜ë¦¬
        if any(word in user_input.lower() for word in ["ìƒˆë¡œ", "ë¦¬ì…‹", "reset", "ë‹¤ì‹œ", "ì²˜ìŒ"]):
            session_states[session_id] = {
                "phase": QuizPhase.SETUP,
                "difficulty": None,
                "subject": None,
                "round_count": 0,
            }
            current_state = session_states[session_id]
            phase = QuizPhase.SETUP
        
        # Handle next question commands / ë‹¤ìŒ ë¬¸ì œ ëª…ë ¹ ì²˜ë¦¬
        if phase == QuizPhase.COMPLETE and any(word in user_input.lower() for word in ["ë‹¤ìŒ", "ê³„ì†", "next", "continue", "ë”"]):
            phase = QuizPhase.QUESTIONING
            current_state["phase"] = phase
        
        # Prepare graph invoke state / ê·¸ë˜í”„ invoke ì¤€ë¹„
        invoke_state = {
            "messages": [HumanMessage(content=user_input)],
            "user_input": user_input,
            "phase": phase,
            "difficulty": current_state.get("difficulty"),
            "subject": current_state.get("subject"),
            "round_count": current_state.get("round_count", 0),
        }
        
        # Execute graph / ê·¸ë˜í”„ ì‹¤í–‰
        result = graph.invoke(invoke_state, config=config)
        
        # Update session state / ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        session_states[session_id] = {
            "phase": result.get("phase", QuizPhase.SETUP),
            "difficulty": result.get("difficulty"),
            "subject": result.get("subject"),
            "round_count": result.get("round_count", 0),
        }
        
        # Collect all new messages / ëª¨ë“  ìƒˆ ë©”ì‹œì§€ ìˆ˜ì§‘
        all_responses = []
        for msg in result["messages"]:
            if hasattr(msg, 'content') and msg.content:
                # Collect only non-HumanMessage / HumanMessageê°€ ì•„ë‹Œ ê²ƒë§Œ ìˆ˜ì§‘
                if not isinstance(msg, HumanMessage):
                    all_responses.append(msg.content)
        
        # Return last AI response (combine if multiple) / ë§ˆì§€ë§‰ AI ì‘ë‹µ ë°˜í™˜
        response_text = "\n\n".join(all_responses) if all_responses else "Unable to generate response. / ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # Background evaluation (non-blocking) / ë°±ê·¸ë¼ìš´ë“œ í‰ê°€ (ë¹„ë™ê¸°, latency ì˜í–¥ ì—†ìŒ)
        asyncio.create_task(
            asyncio.to_thread(evaluate_single, request.message, response_text)
        )
        
        return ChatResponse(response=response_text, session_id=session_id)
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """Stream agent conversation via SSE token-by-token through LangGraph / SSE ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ LangGraphë¥¼ í†µí•œ ì—ì´ì „íŠ¸ ëŒ€í™”ë¥¼ í† í° ë‹¨ìœ„ë¡œ ì‹¤ì‹œê°„ ì „ì†¡"""
    if not graph:
        raise HTTPException(status_code=503, detail="Agent not initialized")
    
    async def event_generator() -> AsyncGenerator[str, None]:
        try:
            # Handle session ID / ì„¸ì…˜ ID ì²˜ë¦¬
            session_id = request.session_id or str(uuid4())
            
            # Get or initialize session state / ì„¸ì…˜ ìƒíƒœ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ì´ˆê¸°í™”
            if session_id not in session_states:
                session_states[session_id] = {
                    "phase": QuizPhase.SETUP,
                    "difficulty": None,
                    "subject": None,
                    "round_count": 0,
                }
            
            current_state = session_states[session_id]
            user_input = request.message.strip()
            
            phase = current_state.get("phase", QuizPhase.SETUP)
            
            # Send session ID / ì„¸ì…˜ ID ì „ì†¡
            yield f"data: {json.dumps({'type': 'session', 'session_id': session_id})}\n\n"
            
            # Handle reset commands / ë¦¬ì…‹ ëª…ë ¹ ì²˜ë¦¬
            if any(word in user_input.lower() for word in ["ìƒˆë¡œ", "ë¦¬ì…‹", "reset", "ë‹¤ì‹œ", "ì²˜ìŒ"]):
                session_states[session_id] = {
                    "phase": QuizPhase.SETUP,
                    "difficulty": None,
                    "subject": None,
                    "round_count": 0,
                }
                current_state = session_states[session_id]
                phase = QuizPhase.SETUP
            
            # Handle next question commands / ë‹¤ìŒ ë¬¸ì œ ëª…ë ¹ ì²˜ë¦¬
            if phase == QuizPhase.COMPLETE and any(word in user_input.lower() for word in ["ë‹¤ìŒ", "ê³„ì†", "next", "continue", "ë”"]):
                phase = QuizPhase.QUESTIONING
                current_state["phase"] = phase
            
            # LangGraph ì„¤ì •
            config = {
                "configurable": {"thread_id": session_id},
            }
            
            # OpenTelemetry span for tracing (Langfuse optimized attributes) / OpenTelemetry spanìœ¼ë¡œ íŠ¸ë ˆì´ì‹±
            with tracer.start_as_current_span("chat_stream") as span:
                # Langfuse Trace-Level Attributes (ë²”ìš©)
                span.set_attribute("langfuse.trace.name", "langgraph-session")
                span.set_attribute("langfuse.session.id", session_id)
                span.set_attribute("langfuse.trace.input", user_input)
            
                # Prepare graph invoke state / ê·¸ë˜í”„ invoke ì¤€ë¹„
                invoke_state = {
                    "messages": [HumanMessage(content=user_input)],
                    "user_input": user_input,
                    "phase": phase,
                    "difficulty": current_state.get("difficulty"),
                    "subject": current_state.get("subject"),
                    "round_count": current_state.get("round_count", 0),
                }
                
                # Use astream for LangGraph execution (stream_mode="updates" for per-node streaming)
                # astreamìœ¼ë¡œ LangGraph ì‹¤í–‰ (stream_mode="updates"ë¡œ ë…¸ë“œë³„ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°)
                current_node = None
                node_labels = {
                    "teacher_question": "ğŸ‘¨â€ğŸ« Teacher (Question) / ğŸ‘¨â€ğŸ« Teacher (ë¬¸ì œ)",
                    "student_answer": "ğŸ§‘â€ğŸ“ Student",
                    "teacher_evaluate": "ğŸ‘¨â€ğŸ« Teacher (Evaluate) / ğŸ‘¨â€ğŸ« Teacher (í‰ê°€)",
                }
                
                # stream_mode="updates" for per-node result streaming
                # Note: traceloop-sdk auto-instruments LLM calls (gen_ai.prompt, gen_ai.completion)
                # Here we only add node-level metadata
                final_output = ""  # Track final output / ìµœì¢… ì¶œë ¥ ì¶”ì ìš©
                async for event in graph.astream(invoke_state, config=config, stream_mode="updates"):
                    for node_name, node_output in event.items():
                        print(f"[DEBUG] node={node_name}, output_keys={node_output.keys() if isinstance(node_output, dict) else 'not dict'}")
                        
                        # Extract messages / ë©”ì‹œì§€ ì¶”ì¶œ
                        if isinstance(node_output, dict) and "messages" in node_output:
                            for msg in node_output["messages"]:
                                if hasattr(msg, "content") and msg.content:
                                    content = msg.content
                                    final_output = content  # Save final output / ìµœì¢… ì¶œë ¥ ì €ì¥
                                    
                                    # Set node-specific labels / ë…¸ë“œë³„ ë¼ë²¨ ì„¤ì •
                                    label = node_labels.get(node_name, node_name)
                                    if node_name == "teacher_question":
                                        rc = current_state.get("round_count", 0) + 1
                                        current_state["round_count"] = rc
                                        label = f"ğŸ‘¨â€ğŸ« Teacher (Question #{rc}) / ğŸ‘¨â€ğŸ« Teacher (ë¬¸ì œ #{rc})"
                                    
                                    # Node start notification / ë…¸ë“œ ì‹œì‘ ì•Œë¦¼
                                    if node_name in node_labels:
                                        yield f"data: {json.dumps({'type': 'node_start', 'node': node_name, 'label': label}, ensure_ascii=False)}\n\n"
                                    
                                    # Send full message (typing effect on frontend) / ì „ì²´ ë©”ì‹œì§€ ì „ì†¡
                                    yield f"data: {json.dumps({'type': 'message', 'node': node_name, 'content': content}, ensure_ascii=False)}\n\n"
                                    
                                    # Node end / ë…¸ë“œ ì¢…ë£Œ
                                    if node_name in node_labels:
                                        yield f"data: {json.dumps({'type': 'node_end', 'node': node_name})}\n\n"
                                    
                                    # Show waiting for next node / ë‹¤ìŒ ë…¸ë“œ ëŒ€ê¸° í‘œì‹œ
                                    if node_name == "setup" and "í€´ì¦ˆ ì„¤ì • ì™„ë£Œ" in content:
                                        yield f"data: {json.dumps({'type': 'waiting', 'message': 'ğŸ‘¨â€ğŸ« Teacher is preparing a question... / Teacherê°€ ë¬¸ì œë¥¼ ì¤€ë¹„ ì¤‘...'})}\n\n"
                                    elif node_name == "teacher_question":
                                        yield f"data: {json.dumps({'type': 'waiting', 'message': 'ğŸ§‘â€ğŸ“ Student is thinking... / Studentê°€ ìƒê° ì¤‘...'})}\n\n"
                                    elif node_name == "student_answer":
                                        yield f"data: {json.dumps({'type': 'waiting', 'message': 'ğŸ‘¨â€ğŸ« Teacher is evaluating... / Teacherê°€ í‰ê°€ ì¤‘...'})}\n\n"
                            
                                    await asyncio.sleep(0.1)
                
                # Set trace output (final response) / Trace output ì„¤ì • (ìµœì¢… ì‘ë‹µ)
                if final_output:
                    span.set_attribute("langfuse.trace.output", final_output[:10000] if len(final_output) > 10000 else final_output)
            
            # Get final state / ìµœì¢… ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
            final_state = graph.get_state(config)
            if final_state and final_state.values:
                session_states[session_id] = {
                    "phase": final_state.values.get("phase", QuizPhase.SETUP),
                    "difficulty": final_state.values.get("difficulty"),
                    "subject": final_state.values.get("subject"),
                    "round_count": final_state.values.get("round_count", 0),
                }
            
            # Background evaluation (non-blocking) / ë°±ê·¸ë¼ìš´ë“œ í‰ê°€ (ë¹„ë™ê¸°, latency ì˜í–¥ ì—†ìŒ)
            if final_output and user_input:
                asyncio.create_task(
                    asyncio.to_thread(evaluate_single, user_input, final_output)
                )
            
            # Done event / ì™„ë£Œ ì´ë²¤íŠ¸
            yield f"data: {json.dumps({'type': 'done'})}\n\n"
            
        except Exception as e:
            print(f"Streaming Error: {e}")
            import traceback
            traceback.print_exc()
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


@app.get("/health")
async def health_check():
    return {"status": "healthy", "graph_initialized": graph is not None}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
