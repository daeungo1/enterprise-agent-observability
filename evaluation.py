"""
Azure AI Evaluation Pipeline for LangGraph Agent
Application Insightsì—ì„œ íŠ¸ë ˆì´ìŠ¤ ë°ì´í„°ë¥¼ ì¿¼ë¦¬í•˜ê³  ìë™ìœ¼ë¡œ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""
import os
import json
import asyncio
from datetime import datetime, timedelta
from typing import Optional
from pathlib import Path

from dotenv import load_dotenv

load_dotenv()

# Azure AI Evaluation SDK
from azure.ai.evaluation import (
    evaluate,
    FluencyEvaluator,
    QAEvaluator,
    AzureOpenAIModelConfiguration,
)

# Azure AI Content Safety
from azure.ai.contentsafety import ContentSafetyClient
from azure.ai.contentsafety.models import AnalyzeTextOptions, TextCategory
from azure.core.credentials import AzureKeyCredential

# Azure Monitor Query for Application Insights
from azure.monitor.query import LogsQueryClient
from azure.identity import DefaultAzureCredential


# Configuration
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_DEPLOYMENT_NAME = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o")
AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2024-08-01-preview")

# Application Insights
APP_INSIGHTS_WORKSPACE_ID = os.getenv("APP_INSIGHTS_WORKSPACE_ID")
APP_INSIGHTS_CONNECTION_STRING = os.getenv("APP_INSIGHTS_CONNECTION_STRING")

# Azure AI Content Safety
AZURE_CONTENT_SAFETY_ENDPOINT = os.getenv("AZURE_CONTENT_SAFETY_ENDPOINT")
AZURE_CONTENT_SAFETY_KEY = os.getenv("AZURE_CONTENT_SAFETY_KEY")

# Output directory
OUTPUT_DIR = Path("evaluation_results")
OUTPUT_DIR.mkdir(exist_ok=True)


def get_model_config() -> AzureOpenAIModelConfiguration:
    """Azure OpenAI ëª¨ë¸ ì„¤ì •"""
    return AzureOpenAIModelConfiguration(
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        api_key=AZURE_OPENAI_API_KEY,
        azure_deployment=AZURE_OPENAI_DEPLOYMENT_NAME,
        api_version=AZURE_OPENAI_API_VERSION,
    )


def query_traces_from_app_insights(
    hours: int = 24,
    limit: int = 100,
) -> list[dict]:
    """
    Application Insightsì—ì„œ LLM íŠ¸ë ˆì´ìŠ¤ ë°ì´í„°ë¥¼ ì¿¼ë¦¬í•©ë‹ˆë‹¤.
    
    Returns:
        list[dict]: query, response, contextë¥¼ í¬í•¨í•œ íŠ¸ë ˆì´ìŠ¤ ë°ì´í„°
    """
    if not APP_INSIGHTS_WORKSPACE_ID:
        print("âš ï¸ APP_INSIGHTS_WORKSPACE_ID not set. Using sample data.")
        return get_sample_data()
    
    credential = DefaultAzureCredential()
    client = LogsQueryClient(credential)
    
    # KQL ì¿¼ë¦¬: AppDependencies í…Œì´ë¸”ì—ì„œ LLM input/output ì¶”ì¶œ
    # OpenTelemetry íŠ¸ë ˆì´ìŠ¤ ë°ì´í„°ëŠ” AppDependenciesì— ì €ì¥ë¨
    # Langfuse/Traceloop SDKê°€ langfuse.trace.input/output í‚¤ë¡œ ì €ì¥
    query = f"""
    AppDependencies
    | where TimeGenerated > ago({hours}h)
    | where Name == "chat_stream" 
        or (Name has_any ("AzureChatOpenAI", "ChatOpenAI") and Properties has "gen_ai.prompt")
    | extend 
        query = coalesce(
            tostring(Properties['langfuse.trace.input']),
            tostring(Properties['gen_ai.prompt']),
            tostring(Properties['llm.prompts']),
            tostring(Properties['traceloop.entity.input']),
            ""
        ),
        response = coalesce(
            tostring(Properties['langfuse.trace.output']),
            tostring(Properties['gen_ai.completion']),
            tostring(Properties['llm.completions']),
            tostring(Properties['traceloop.entity.output']),
            ""
        ),
        context = ""
    | where isnotempty(query) and isnotempty(response)
    | project TimeGenerated, OperationId, query, response, context, Name
    | order by TimeGenerated desc
    | limit {limit}
    """
    
    try:
        response = client.query_workspace(
            workspace_id=APP_INSIGHTS_WORKSPACE_ID,
            query=query,
            timespan=timedelta(hours=hours),
        )
        
        traces = []
        for table in response.tables:
            for row in table.rows:
                traces.append({
                    "timestamp": str(row[0]),
                    "operation_id": str(row[1]),
                    "query": str(row[2]) if row[2] else "",
                    "response": str(row[3]) if row[3] else "",
                    "context": str(row[4]) if row[4] else "",
                })
        
        if traces:
            print(f"âœ… Queried {len(traces)} traces from Application Insights (AppDependencies)")
            return traces
        
        # AppDependenciesì— ì—†ìœ¼ë©´ AppEvents ì‹œë„
        print("   No traces found in 'AppDependencies'. Trying 'AppEvents'...")
        return query_from_app_events(client, hours, limit)
        
    except Exception as e:
        print(f"âŒ Error querying Application Insights: {e}")
        print("Using sample data instead.")
        return get_sample_data()


def query_from_app_events(client: LogsQueryClient, hours: int, limit: int) -> list[dict]:
    """AppEvents í…Œì´ë¸”ì—ì„œ LLM íŠ¸ë ˆì´ìŠ¤ ì¿¼ë¦¬"""
    query = f"""
    AppEvents
    | where TimeGenerated > ago({hours}h)
    | where Name has_any ("llm", "chat", "openai", "langgraph", "teacher", "student", "evaluation")
        or Properties has_any ("llm", "prompt", "completion", "query", "response")
    | extend 
        query = coalesce(
            tostring(Properties['llm.prompts']),
            tostring(Properties['gen_ai.prompt']),
            tostring(Properties['query']),
            tostring(Properties['input']),
            Name
        ),
        response = coalesce(
            tostring(Properties['llm.completions']),
            tostring(Properties['gen_ai.completion']),
            tostring(Properties['response']),
            tostring(Properties['output']),
            ""
        )
    | where isnotempty(query)
    | project TimeGenerated, OperationId, query, response
    | order by TimeGenerated desc
    | limit {limit}
    """
    
    try:
        response = client.query_workspace(
            workspace_id=APP_INSIGHTS_WORKSPACE_ID,
            query=query,
            timespan=timedelta(hours=hours),
        )
        
        traces = []
        for table in response.tables:
            for row in table.rows:
                traces.append({
                    "timestamp": str(row[0]),
                    "operation_id": str(row[1]),
                    "query": str(row[2]) if row[2] else "",
                    "response": str(row[3]) if row[3] else "",
                    "context": "",
                })
        
        if traces:
            print(f"âœ… Queried {len(traces)} traces from Application Insights (AppEvents)")
            return traces
        
        print("   No traces found. Using sample data.")
        return get_sample_data()
        
    except Exception as e:
        print(f"âŒ Error querying AppEvents: {e}")
        return get_sample_data()
        print("Using sample data instead.")
        return get_sample_data()


def get_sample_data() -> list[dict]:
    """ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ë°ì´í„°"""
    return [
        {
            "query": "ë³´í†µ ìˆ˜í•™ ë¬¸ì œ í’€ë˜",
            "response": "ğŸ‘¨â€ğŸ« **Teacher (ë¬¸ì œ #1)**\n\në‹¤ìŒ ë°©ì •ì‹ì„ í’€ì–´ë³´ì„¸ìš”: 2x + 5 = 15",
            "context": "Teacher-Student í€´ì¦ˆ ì‹œìŠ¤í…œ",
            "ground_truth": "x = 5",
        },
        {
            "query": "ì‰¬ìš´ ì—­ì‚¬ í€´ì¦ˆ",
            "response": "ğŸ‘¨â€ğŸ« **Teacher (ë¬¸ì œ #1)**\n\nëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
            "context": "Teacher-Student í€´ì¦ˆ ì‹œìŠ¤í…œ",
            "ground_truth": "ì„œìš¸",
        },
        {
            "query": "í”„ë¡œê·¸ë˜ë° ë¬¸ì œ ì¶œì œí•´ì¤˜",
            "response": "ğŸ‘¨â€ğŸ« **Teacher (ë¬¸ì œ #1)**\n\nPythonì—ì„œ ë¦¬ìŠ¤íŠ¸ì˜ ëª¨ë“  ìš”ì†Œë¥¼ í•©í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”.",
            "context": "Teacher-Student í€´ì¦ˆ ì‹œìŠ¤í…œ",
            "ground_truth": "sum() í•¨ìˆ˜ ì‚¬ìš© ë˜ëŠ” for ë£¨í”„",
        },
    ]


def save_traces_as_jsonl(traces: list[dict], filename: str) -> Path:
    """íŠ¸ë ˆì´ìŠ¤ë¥¼ JSONL íŒŒì¼ë¡œ ì €ì¥ (í‰ê°€ì— í•„ìš”í•œ í•„ë“œë§Œ)"""
    filepath = OUTPUT_DIR / filename
    with open(filepath, "w", encoding="utf-8") as f:
        for trace in traces:
            # í‰ê°€ì— í•„ìš”í•œ í•„ë“œë§Œ ì¶”ì¶œ (timestamp ì œì™¸)
            eval_data = {
                "query": str(trace.get("query", "")),
                "response": str(trace.get("response", "")),
                "context": str(trace.get("context", "")),
            }
            f.write(json.dumps(eval_data, ensure_ascii=False) + "\n")
    print(f"âœ… Saved {len(traces)} traces to {filepath}")
    return filepath


def run_quality_evaluation(data_path: Path) -> dict:
    """
    í’ˆì§ˆ í‰ê°€ ì‹¤í–‰ (Fluency, QA)
    """
    model_config = get_model_config()
    
    # Quality Evaluators
    fluency_eval = FluencyEvaluator(model_config)
    
    print("ğŸ” Running Quality Evaluation...")
    
    output_file = OUTPUT_DIR / "quality_evaluation_result.json"
    
    result = evaluate(
        data=str(data_path),
        evaluators={
            "fluency": fluency_eval,
        },
        evaluator_config={
            "fluency": {
                "column_mapping": {
                    "query": "${data.query}",
                    "response": "${data.response}",
                }
            },
        },
        output_path=str(output_file),
    )
    
    print("âœ… Quality Evaluation completed")
    
    # ê²°ê³¼ íŒŒì¼ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ (Timestamp ì§ë ¬í™” ë¬¸ì œ ìš°íšŒ)
    try:
        with open(output_file, "r", encoding="utf-8") as f:
            saved_result = json.load(f)
        
        # metrics ê³„ì‚°
        rows = saved_result.get("rows", [])
        fluency_scores = [r.get("outputs.fluency.fluency", 0) for r in rows if r.get("outputs.fluency.fluency") is not None]
        
        metrics = {
            "fluency.fluency": sum(fluency_scores) / len(fluency_scores) if fluency_scores else None
        }
        
        return {"metrics": metrics, "rows": rows}
    except Exception as e:
        print(f"   Warning: Could not parse saved result: {e}")
        return {"metrics": {}, "rows": []}


def run_safety_evaluation(traces: list[dict]) -> dict:
    """
    Azure AI Content Safetyë¥¼ ì‚¬ìš©í•œ ì•ˆì „ì„± í‰ê°€
    Violence, Hate, Sexual, SelfHarm ì¹´í…Œê³ ë¦¬ ë¶„ì„
    """
    if not AZURE_CONTENT_SAFETY_ENDPOINT or not AZURE_CONTENT_SAFETY_KEY:
        print("âš ï¸ Azure AI Content Safety not configured. Skipping safety evaluation.")
        return {"metrics": {}, "rows": []}
    
    print("ğŸ” Running Safety Evaluation (Azure AI Content Safety)...")
    
    # Content Safety Client ìƒì„±
    client = ContentSafetyClient(
        endpoint=AZURE_CONTENT_SAFETY_ENDPOINT,
        credential=AzureKeyCredential(AZURE_CONTENT_SAFETY_KEY)
    )
    
    # ê° ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ì§‘ê³„
    violence_scores = []
    hate_scores = []
    sexual_scores = []
    self_harm_scores = []
    rows = []
    
    for i, trace in enumerate(traces):
        try:
            # query + responseë¥¼ í•©ì³ì„œ ë¶„ì„
            text_to_analyze = f"{trace.get('query', '')} {trace.get('response', '')}"
            
            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸° (API ì œí•œ)
            if len(text_to_analyze) > 10000:
                text_to_analyze = text_to_analyze[:10000]
            
            # Content Safety ë¶„ì„ ìš”ì²­
            request = AnalyzeTextOptions(text=text_to_analyze)
            response = client.analyze_text(request)
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ì¶”ì¶œ (0-6 ìŠ¤ì¼€ì¼, 0=ì•ˆì „, 6=ìœ„í—˜)
            row_result = {
                "query": trace.get("query", "")[:200],
                "violence": 0,
                "hate": 0,
                "sexual": 0,
                "self_harm": 0,
            }
            
            for category_result in response.categories_analysis:
                severity = category_result.severity or 0
                if category_result.category == TextCategory.VIOLENCE:
                    row_result["violence"] = severity
                    violence_scores.append(severity)
                elif category_result.category == TextCategory.HATE:
                    row_result["hate"] = severity
                    hate_scores.append(severity)
                elif category_result.category == TextCategory.SEXUAL:
                    row_result["sexual"] = severity
                    sexual_scores.append(severity)
                elif category_result.category == TextCategory.SELF_HARM:
                    row_result["self_harm"] = severity
                    self_harm_scores.append(severity)
            
            rows.append(row_result)
            
            if (i + 1) % 10 == 0:
                print(f"   Processed {i + 1}/{len(traces)} samples...")
                
        except Exception as e:
            print(f"   âš ï¸ Error analyzing sample {i + 1}: {e}")
            continue
    
    # í‰ê·  ì ìˆ˜ ê³„ì‚°
    metrics = {
        "safety_violence": round(sum(violence_scores) / len(violence_scores), 2) if violence_scores else None,
        "safety_hate_unfairness": round(sum(hate_scores) / len(hate_scores), 2) if hate_scores else None,
        "safety_sexual": round(sum(sexual_scores) / len(sexual_scores), 2) if sexual_scores else None,
        "safety_self_harm": round(sum(self_harm_scores) / len(self_harm_scores), 2) if self_harm_scores else None,
    }
    
    # ê²°ê³¼ ì €ì¥
    result = {"metrics": metrics, "rows": rows}
    with open(OUTPUT_DIR / "safety_evaluation_result.json", "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… Safety Evaluation completed ({len(rows)} samples)")
    return result


def run_qa_evaluation(data_path: Path) -> dict:
    """
    QA í‰ê°€ ì‹¤í–‰ (Groundedness, Relevance, Coherence, Fluency, Similarity, F1)
    """
    model_config = get_model_config()
    
    # QA Evaluator (ë³µí•© í‰ê°€ì)
    qa_eval = QAEvaluator(model_config)
    
    print("ğŸ” Running QA Evaluation...")
    
    output_file = OUTPUT_DIR / "qa_evaluation_result.json"
    
    result = evaluate(
        data=str(data_path),
        evaluators={
            "qa": qa_eval,
        },
        evaluator_config={
            "qa": {
                "column_mapping": {
                    "query": "${data.query}",
                    "response": "${data.response}",
                    "context": "${data.context}",
                    "ground_truth": "${data.ground_truth}",
                }
            },
        },
        output_path=str(output_file),
    )
    
    print("âœ… QA Evaluation completed")
    
    # ê²°ê³¼ íŒŒì¼ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ (Timestamp ì§ë ¬í™” ë¬¸ì œ ìš°íšŒ)
    try:
        with open(output_file, "r", encoding="utf-8") as f:
            saved_result = json.load(f)
        
        # metrics ê³„ì‚°
        rows = saved_result.get("rows", [])
        coherence_scores = [r.get("outputs.qa.coherence", 0) for r in rows if r.get("outputs.qa.coherence") is not None]
        relevance_scores = [r.get("outputs.qa.relevance", 0) for r in rows if r.get("outputs.qa.relevance") is not None]
        groundedness_scores = [r.get("outputs.qa.groundedness", 0) for r in rows if r.get("outputs.qa.groundedness") is not None]
        
        metrics = {
            "qa.coherence": sum(coherence_scores) / len(coherence_scores) if coherence_scores else None,
            "qa.relevance": sum(relevance_scores) / len(relevance_scores) if relevance_scores else None,
            "qa.groundedness": sum(groundedness_scores) / len(groundedness_scores) if groundedness_scores else None,
        }
        
        return {"metrics": metrics, "rows": rows}
    except Exception as e:
        print(f"   Warning: Could not parse saved result: {e}")
        return {"metrics": {}, "rows": []}


def generate_evaluation_summary(results: dict) -> dict:
    """í‰ê°€ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
    summary = {
        "timestamp": datetime.now().isoformat(),
        "metrics": results.get("metrics", {}),
        "total_samples": len(results.get("rows", [])),
    }
    
    # ë©”íŠ¸ë¦­ë³„ í†µê³„
    metrics = results.get("metrics", {})
    summary["quality_scores"] = {
        "fluency": metrics.get("fluency.fluency", None),
        "coherence": metrics.get("qa.coherence", None),
        "relevance": metrics.get("qa.relevance", None),
        "groundedness": metrics.get("qa.groundedness", None),
    }
    
    # Safety scores - Azure AI Content Safety í˜•ì‹ (safety_*)
    summary["safety_scores"] = {
        "violence": metrics.get("safety_violence", None),
        "sexual": metrics.get("safety_sexual", None),
        "self_harm": metrics.get("safety_self_harm", None),
        "hate_unfairness": metrics.get("safety_hate_unfairness", None),
    }
    
    return summary


def save_evaluation_for_grafana(summary: dict, filename: str = "evaluation_metrics.json"):
    """Grafanaì—ì„œ ì½ì„ ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
    filepath = OUTPUT_DIR / filename
    
    # Grafana JSON ë°ì´í„°ì†ŒìŠ¤ í˜•ì‹
    grafana_data = {
        "timestamp": summary["timestamp"],
        "total_samples": summary["total_samples"],
        **{f"quality_{k}": v for k, v in summary["quality_scores"].items() if v is not None},
        **{f"safety_{k}": v for k, v in summary["safety_scores"].items() if v is not None},
    }
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(grafana_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… Saved Grafana metrics to {filepath}")
    return filepath


def send_evaluation_to_app_insights(summary: dict, rows: list[dict] = None):
    """
    í‰ê°€ ê²°ê³¼ë¥¼ Application Insightsë¡œ ì „ì†¡ (customEvents)
    Grafana ëŒ€ì‹œë³´ë“œì—ì„œ ì¡°íšŒ ê°€ëŠ¥
    """
    if not APP_INSIGHTS_CONNECTION_STRING:
        print("âš ï¸ APP_INSIGHTS_CONNECTION_STRING not set. Skipping telemetry.")
        return
    
    try:
        from opencensus.ext.azure import metrics_exporter
        from opencensus.ext.azure.log_exporter import AzureEventHandler
        import logging
        
        # Azure Event Handler ì„¤ì •
        logger = logging.getLogger("evaluation")
        logger.setLevel(logging.INFO)
        
        handler = AzureEventHandler(connection_string=APP_INSIGHTS_CONNECTION_STRING)
        logger.addHandler(handler)
        
        # ìš”ì•½ ë©”íŠ¸ë¦­ ì „ì†¡
        event_properties = {
            "total_samples": str(summary["total_samples"]),
            **{f"quality_{k}": str(v) if v is not None else "null" 
               for k, v in summary["quality_scores"].items()},
            **{f"safety_{k}": str(v) if v is not None else "null" 
               for k, v in summary["safety_scores"].items()},
        }
        
        logger.info("evaluation_result", extra={"custom_dimensions": event_properties})
        
        # ê°œë³„ í–‰ ê²°ê³¼ ì „ì†¡ (ì„ íƒì )
        if rows:
            for row in rows[:50]:  # ìµœëŒ€ 50ê°œê¹Œì§€ë§Œ
                row_properties = {
                    "query": str(row.get("inputs.query", ""))[:500],
                    "response": str(row.get("inputs.response", ""))[:500],
                    **{k: str(v) for k, v in row.items() if k.startswith("outputs.")}
                }
                logger.info("evaluation_row", extra={"custom_dimensions": row_properties})
        
        print("âœ… Sent evaluation results to Application Insights")
        
    except ImportError:
        print("âš ï¸ opencensus-ext-azure not installed. Using OpenTelemetry instead.")
        send_evaluation_via_otel(summary, rows)
    except Exception as e:
        print(f"âš ï¸ Failed to send to Application Insights: {e}")


def send_evaluation_via_otel(summary: dict, rows: list[dict] = None):
    """
    OpenTelemetryë¥¼ í†µí•´ í‰ê°€ ê²°ê³¼ ì „ì†¡ (OTel Collector â†’ App Insights)
    """
    try:
        from opentelemetry import trace
        from opentelemetry.sdk.trace import TracerProvider
        from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
        from opentelemetry.sdk.trace.export import BatchSpanProcessor
        
        from config import OTEL_EXPORTER_OTLP_ENDPOINT
        
        # Tracer ì„¤ì •
        provider = TracerProvider()
        exporter = OTLPSpanExporter(endpoint=OTEL_EXPORTER_OTLP_ENDPOINT, insecure=True)
        provider.add_span_processor(BatchSpanProcessor(exporter))
        trace.set_tracer_provider(provider)
        
        tracer = trace.get_tracer("evaluation-pipeline")
        
        # í‰ê°€ ê²°ê³¼ span ìƒì„±
        with tracer.start_as_current_span("evaluation_result") as span:
            span.set_attribute("total_samples", summary["total_samples"])
            
            for k, v in summary["quality_scores"].items():
                if v is not None:
                    span.set_attribute(f"quality_{k}", v)
            
            for k, v in summary["safety_scores"].items():
                if v is not None:
                    span.set_attribute(f"safety_{k}", v)
        
        print("âœ… Sent evaluation results via OpenTelemetry")
        
    except Exception as e:
        print(f"âš ï¸ Failed to send via OpenTelemetry: {e}")


async def run_evaluation_pipeline(hours: int = 24, limit: int = 100):
    """
    ì „ì²´ í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    
    Args:
        hours: ì¿¼ë¦¬í•  ì‹œê°„ ë²”ìœ„ (ì‹œê°„)
        limit: ìµœëŒ€ íŠ¸ë ˆì´ìŠ¤ ìˆ˜
    """
    print("=" * 60)
    print("ğŸš€ Starting Evaluation Pipeline")
    print(f"   Time Range: Last {hours} hours")
    print(f"   Max Samples: {limit}")
    print("=" * 60)
    
    # 1. Application Insightsì—ì„œ íŠ¸ë ˆì´ìŠ¤ ì¿¼ë¦¬
    traces = query_traces_from_app_insights(hours=hours, limit=limit)
    
    if not traces:
        print("âŒ No traces found. Exiting.")
        return
    
    # 2. JSONL íŒŒì¼ë¡œ ì €ì¥
    data_path = save_traces_as_jsonl(traces, "evaluation_data.jsonl")
    
    # 3. í‰ê°€ ì‹¤í–‰
    all_results = {"metrics": {}, "rows": []}
    
    # Quality Evaluation
    try:
        quality_result = run_quality_evaluation(data_path)
        all_results["metrics"].update(quality_result.get("metrics", {}))
        all_results["rows"].extend(quality_result.get("rows", []))
    except Exception as e:
        print(f"âš ï¸ Quality evaluation error: {e}")
        # ê²°ê³¼ íŒŒì¼ì—ì„œ ì§ì ‘ ì½ê¸° ì‹œë„
        try:
            quality_file = OUTPUT_DIR / "quality_evaluation_result.json"
            if quality_file.exists():
                with open(quality_file, "r", encoding="utf-8") as f:
                    saved_result = json.load(f)
                rows = saved_result.get("rows", [])
                fluency_scores = [r.get("outputs.fluency.fluency", 0) for r in rows if r.get("outputs.fluency.fluency") is not None]
                if fluency_scores:
                    all_results["metrics"]["fluency.fluency"] = sum(fluency_scores) / len(fluency_scores)
                    all_results["rows"].extend(rows)
                    print(f"   âœ… Recovered quality metrics from saved file")
        except Exception as e2:
            print(f"   Could not recover from file: {e2}")
    
    # Safety Evaluation (Azure AI Content Safety ì‚¬ìš©)
    try:
        safety_result = run_safety_evaluation(traces)
        all_results["metrics"].update(safety_result.get("metrics", {}))
    except Exception as e:
        print(f"âš ï¸ Safety evaluation failed: {e}")
    
    # QA Evaluation (ground_truthê°€ ìˆëŠ” ê²½ìš°)
    if any("ground_truth" in t for t in traces):
        try:
            qa_result = run_qa_evaluation(data_path)
            all_results["metrics"].update(qa_result.get("metrics", {}))
        except Exception as e:
            print(f"âš ï¸ QA evaluation failed: {e}")
    
    # 4. ê²°ê³¼ ìš”ì•½
    summary = generate_evaluation_summary(all_results)
    
    # 5. Grafanaìš© ì €ì¥
    save_evaluation_for_grafana(summary)
    
    # 6. Application Insightsë¡œ ì „ì†¡ (Grafana ëŒ€ì‹œë³´ë“œìš©)
    send_evaluation_to_app_insights(summary, all_results.get("rows", []))
    
    # 7. ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š Evaluation Summary")
    print("=" * 60)
    print(f"Total Samples: {summary['total_samples']}")
    print("\nğŸ¯ Quality Scores:")
    for k, v in summary["quality_scores"].items():
        if v is not None:
            print(f"   {k}: {v:.2f}")
    print("\nğŸ›¡ï¸ Safety Scores:")
    for k, v in summary["safety_scores"].items():
        if v is not None:
            print(f"   {k}: {v:.2f}")
    print("=" * 60)
    
    return summary


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description="LangGraph Agent Evaluation Pipeline")
    parser.add_argument("--hours", type=int, default=24, help="Time range in hours (default: 24)")
    parser.add_argument("--limit", type=int, default=100, help="Max traces to evaluate (default: 100)")
    args = parser.parse_args()
    
    asyncio.run(run_evaluation_pipeline(hours=args.hours, limit=args.limit))


if __name__ == "__main__":
    main()
